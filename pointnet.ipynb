{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "- https://keras.io/examples/vision/pointnet_segmentation/\n",
    "\n",
    "NOTE:\n",
    "- Several links in the link and comments that may be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from temp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/.keras/datasets/PartAnnotation/metadata.json\") as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points\".format(\n",
    "    metadata[\"Airplane\"][\"directory\"]\n",
    ")\n",
    "labels_dir = \"/tmp/.keras/datasets/PartAnnotation/{}/points_label\".format(\n",
    "    metadata[\"Airplane\"][\"directory\"]\n",
    ")\n",
    "LABELS = metadata[\"Airplane\"][\"lables\"]\n",
    "COLORS = metadata[\"Airplane\"][\"colors\"]\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "NUM_SAMPLE_POINTS = 1024\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 60\n",
    "INITIAL_LR = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clouds = 100\n",
    "\n",
    "point_clouds = []\n",
    "point_cloud_labels = []\n",
    "all_labels = []\n",
    "\n",
    "points_files = glob(os.path.join(points_dir, \"*.pts\"))\n",
    "\n",
    "for idx, point_file in tqdm(enumerate(points_files)):\n",
    "    if idx == num_clouds:\n",
    "        break\n",
    "\n",
    "    point_file = point_file.replace(\"\\\\\", \"/\")\n",
    "    point_cloud = np.loadtxt(point_file)\n",
    "    if point_cloud.shape[0] < NUM_SAMPLE_POINTS:\n",
    "        continue\n",
    "\n",
    "    # Get the file-id of the current point cloud for parsing its\n",
    "    # labels.\n",
    "    file_id = point_file.split(\"/\")[-1].split(\".\")[0]\n",
    "    label_data, num_labels = {}, 0\n",
    "    for label in LABELS:\n",
    "        label_file = os.path.join(labels_dir, label, file_id + \".seg\")\n",
    "        if os.path.exists(label_file):\n",
    "            label_data[label] = np.loadtxt(label_file).astype(\"float32\")\n",
    "            num_labels = len(label_data[label])\n",
    "\n",
    "    # Point clouds having labels will be our training samples.\n",
    "    try:\n",
    "        label_map = [\"none\"] * num_labels\n",
    "        for label in LABELS:\n",
    "            for i, data in enumerate(label_data[label]):\n",
    "                label_map[i] = label if data == 1 else label_map[i]\n",
    "        label_data = [\n",
    "            LABELS.index(label) if label != \"none\" else len(LABELS)\n",
    "            for label in label_map\n",
    "        ]\n",
    "        # Apply one-hot encoding to the dense label representation.\n",
    "        label_data = keras.utils.to_categorical(label_data, num_classes=len(LABELS) + 1)\n",
    "\n",
    "        point_clouds.append(point_cloud)\n",
    "        point_cloud_labels.append(label_data)\n",
    "        all_labels.append(label_map)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    i = random.randint(0, len(point_clouds) - 1)\n",
    "    print(f\"point_clouds[{i}].shape:\", point_clouds[0].shape)\n",
    "    print(f\"point_cloud_labels[{i}].shape:\", point_cloud_labels[0].shape)\n",
    "    for j in range(5):\n",
    "        print(\n",
    "            f\"all_labels[{i}][{j}]:\",\n",
    "            all_labels[i][j],\n",
    "            f\"\\tpoint_cloud_labels[{i}][{j}]:\",\n",
    "            point_cloud_labels[i][j],\n",
    "            \"\\n\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(point_cloud, labels):\n",
    "    df = pd.DataFrame(\n",
    "        data={\n",
    "            \"x\": point_cloud[:, 0],\n",
    "            \"y\": point_cloud[:, 1],\n",
    "            \"z\": point_cloud[:, 2],\n",
    "            \"label\": labels,\n",
    "        }\n",
    "    )\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    for index, label in enumerate(LABELS):\n",
    "        c_df = df[df[\"label\"] == label]\n",
    "        try:\n",
    "            ax.scatter(\n",
    "                c_df[\"x\"], c_df[\"y\"], c_df[\"z\"], label=label, alpha=0.5, c=COLORS[index]\n",
    "            )\n",
    "        except IndexError:\n",
    "            pass\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_data(point_clouds[0], all_labels[0])\n",
    "#visualize_data(point_clouds[300], all_labels[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Why does normalizing reduce the amount of points?\n",
    "- Which points are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(point_clouds))\n",
    "print(len(point_cloud_labels))\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in tqdm(range(len(point_clouds))):\n",
    "    current_point_cloud = point_clouds[index]\n",
    "    current_label_cloud = point_cloud_labels[index]\n",
    "    current_labels = all_labels[index]\n",
    "    num_points = len(current_point_cloud)\n",
    "\n",
    "    # Randomly sampling respective indices.\n",
    "    sampled_indices = random.sample(list(range(num_points)), NUM_SAMPLE_POINTS)\n",
    "    # Sampling points corresponding to sampled indices.\n",
    "    sampled_point_cloud = np.array([current_point_cloud[i] for i in sampled_indices])\n",
    "    # Sampling corresponding one-hot encoded labels.\n",
    "    sampled_label_cloud = np.array([current_label_cloud[i] for i in sampled_indices])\n",
    "    # Sampling corresponding labels for visualization.\n",
    "    sampled_labels = np.array([current_labels[i] for i in sampled_indices])\n",
    "    # Normalizing sampled point cloud.\n",
    "    norm_point_cloud = sampled_point_cloud - np.mean(sampled_point_cloud, axis=0)\n",
    "    norm_point_cloud /= np.max(np.linalg.norm(norm_point_cloud, axis=1))\n",
    "\n",
    "    point_clouds[index] = norm_point_cloud\n",
    "    point_cloud_labels[index] = sampled_label_cloud\n",
    "    all_labels[index] = sampled_labels\n",
    "\n",
    "print(\"point_clouds[0].shape:\", point_clouds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(point_clouds[0], all_labels[0])\n",
    "#visualize_data(point_clouds[300], all_labels[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_labels[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Why do we need to generate a trainin set?\n",
    "- How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(point_cloud_batch, label_cloud_batch):\n",
    "    point_cloud_batch.set_shape([NUM_SAMPLE_POINTS, 3])\n",
    "    label_cloud_batch.set_shape([NUM_SAMPLE_POINTS, len(LABELS) + 1])\n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "\n",
    "def augment(point_cloud_batch, label_cloud_batch):\n",
    "    noise = tf.random.uniform(\n",
    "        tf.shape(label_cloud_batch), -0.005, 0.005, dtype=tf.float64\n",
    "    )\n",
    "    point_cloud_batch += noise[:, :, :3]\n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "\n",
    "def generate_dataset(point_clouds, label_clouds, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n",
    "    dataset = dataset.shuffle(BATCH_SIZE * 100) if is_training else dataset\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
    "    dataset = (\n",
    "        dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if is_training\n",
    "        else dataset\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "split_index = int(len(point_clouds) * (1 - VAL_SPLIT))\n",
    "\n",
    "train_point_clouds = point_clouds[:split_index]\n",
    "train_label_cloud = point_cloud_labels[:split_index]\n",
    "\n",
    "val_point_clouds = point_clouds[split_index:]\n",
    "val_label_cloud = point_cloud_labels[split_index:]\n",
    "\n",
    "print(\"Num train point clouds:\", len(train_point_clouds))\n",
    "print(\"Num train point cloud labels:\", len(train_label_cloud))\n",
    "print(\"Num val point clouds:\", len(val_point_clouds))\n",
    "print(\"Num val point cloud labels:\", len(val_label_cloud))\n",
    "\n",
    "train_dataset = generate_dataset(train_point_clouds, train_label_cloud)\n",
    "val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False)\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Validation Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in train_dataset:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Creating datasets from whole point clouds - Total: 36xx\n",
    "\n",
    "- One point cloud is already normalized to 1024 instead of the original 27xx\n",
    "-> Can this be avoided? How are all points included then?\n",
    "-> What do we feed the model with later? And can we visualize the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Why are there so many steps in this model?\n",
    "- What does each step do exactly?\n",
    "- Can I simplify this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x: tf.Tensor, filters: int, name: str) -> tf.Tensor:\n",
    "    x = keras.layers.Conv1D(filters, kernel_size=1, padding=\"valid\", name=f\"{name}_conv\")(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.0, name=f\"{name}_batch_norm\")(x)\n",
    "    return keras.layers.Activation(\"relu\", name=f\"{name}_relu\")(x)\n",
    "\n",
    "\n",
    "def mlp_block(x: tf.Tensor, filters: int, name: str) -> tf.Tensor:\n",
    "    x = keras.layers.Dense(filters, name=f\"{name}_dense\")(x)\n",
    "    x = keras.layers.BatchNormalization(momentum=0.0, name=f\"{name}_batch_norm\")(x)\n",
    "    return keras.layers.Activation(\"relu\", name=f\"{name}_relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    \"\"\"Reference: https://keras.io/examples/vision/pointnet/#build-a-model\"\"\"\n",
    "\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.identity = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.identity))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(keras.layers.TransformerEncoder, self).get_config()\n",
    "        config.update({\"num_features\": self.num_features, \"l2reg_strength\": self.l2reg})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_net(inputs: tf.Tensor, num_features: int, name: str) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Reference: https://keras.io/examples/vision/pointnet/#build-a-model.\n",
    "\n",
    "    The `filters` values come from the original paper:\n",
    "    https://arxiv.org/abs/1612.00593.\n",
    "    \"\"\"\n",
    "    x = conv_block(inputs, filters=64, name=f\"{name}_1\")\n",
    "    x = conv_block(x, filters=128, name=f\"{name}_2\")\n",
    "    x = conv_block(x, filters=1024, name=f\"{name}_3\")\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = mlp_block(x, filters=512, name=f\"{name}_1_1\")\n",
    "    x = mlp_block(x, filters=256, name=f\"{name}_2_1\")\n",
    "    return keras.layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=keras.initializers.Constant(np.eye(num_features).flatten()),\n",
    "        activity_regularizer=OrthogonalRegularizer(num_features),\n",
    "        name=f\"{name}_final\",\n",
    "    )(x)\n",
    "\n",
    "\n",
    "def transformation_block(inputs: tf.Tensor, num_features: int, name: str) -> tf.Tensor:\n",
    "    transformed_features = transformation_net(inputs, num_features, name=name)\n",
    "    transformed_features = keras.layers.Reshape((num_features, num_features))(\n",
    "        transformed_features\n",
    "    )\n",
    "    return keras.layers.Dot(axes=(2, 1), name=f\"{name}_mm\")([inputs, transformed_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_segmentation_model(num_points: int, num_classes: int) -> keras.Model:\n",
    "    input_points = keras.Input(shape=(None, 3))\n",
    "\n",
    "    # PointNet Classification Network.\n",
    "    transformed_inputs = transformation_block(\n",
    "        input_points, num_features=3, name=\"input_transformation_block\"\n",
    "    )\n",
    "    features_64 = conv_block(transformed_inputs, filters=64, name=\"features_64\")\n",
    "    features_128_1 = conv_block(features_64, filters=128, name=\"features_128_1\")\n",
    "    features_128_2 = conv_block(features_128_1, filters=128, name=\"features_128_2\")\n",
    "    transformed_features = transformation_block(\n",
    "        features_128_2, num_features=128, name=\"transformed_features\"\n",
    "    )\n",
    "    features_512 = conv_block(transformed_features, filters=512, name=\"features_512\")\n",
    "    features_2048 = conv_block(features_512, filters=2048, name=\"pre_maxpool_block\")\n",
    "    global_features = keras.layers.MaxPool1D(pool_size=num_points, name=\"global_features\")(\n",
    "        features_2048\n",
    "    )\n",
    "    global_features = tf.tile(global_features, [1, num_points, 1])\n",
    "\n",
    "    # Segmentation head.\n",
    "    segmentation_input = keras.layers.Concatenate(name=\"segmentation_input\")(\n",
    "        [\n",
    "            features_64,\n",
    "            features_128_1,\n",
    "            features_128_2,\n",
    "            transformed_features,\n",
    "            features_512,\n",
    "            global_features,\n",
    "        ]\n",
    "    )\n",
    "    segmentation_features = conv_block(\n",
    "        segmentation_input, filters=128, name=\"segmentation_features\"\n",
    "    )\n",
    "    outputs = keras.layers.Conv1D(\n",
    "        num_classes, kernel_size=1, activation=\"softmax\", name=\"segmentation_head\"\n",
    "    )(segmentation_features)\n",
    "    return keras.Model(input_points, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "num_points = x.shape[1]\n",
    "num_classes = y.shape[-1]\n",
    "\n",
    "segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n",
    "segmentation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(epochs):\n",
    "\n",
    "    segmentation_model = get_shape_segmentation_model(num_points, num_classes)\n",
    "    segmentation_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history = segmentation_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=3,\n",
    "    )\n",
    "\n",
    "    return segmentation_model, history\n",
    "\n",
    "\n",
    "segmentation_model, history = run_experiment(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Can I make use of this visualization in my own project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = next(iter(val_dataset))\n",
    "val_predictions = segmentation_model.predict(validation_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = next(iter(val_dataset))\n",
    "val_predictions = segmentation_model.predict(validation_batch[0])\n",
    "print(f\"Validation prediction shape: {val_predictions.shape}\")\n",
    "\n",
    "\n",
    "def visualize_single_point_cloud(point_clouds, label_clouds, idx):\n",
    "    label_map = LABELS + [\"none\"]\n",
    "    point_cloud = point_clouds[idx]\n",
    "    label_cloud = label_clouds[idx]\n",
    "    visualize_data(point_cloud, [label_map[np.argmax(label)] for label in label_cloud])\n",
    "\n",
    "\n",
    "idx = np.random.choice(len(validation_batch[0]))\n",
    "print(f\"Index selected: {idx}\")\n",
    "\n",
    "# Plotting with ground-truth.\n",
    "visualize_single_point_cloud(validation_batch[0], validation_batch[1], idx)\n",
    "\n",
    "# Plotting with predicted labels.\n",
    "visualize_single_point_cloud(validation_batch[0], val_predictions, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "- Exchange these plots with voxel grid vizualization instead?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "773605ce8f27446b9c69ae732168258e47ea10fbda5833a68cc76e4c6c0866c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
